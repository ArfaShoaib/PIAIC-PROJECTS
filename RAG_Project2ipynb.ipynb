{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8XaiWdCE9UVm2thBTVy7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArfaShoaib/PIAIC-PROJECTS/blob/main/RAG_Project2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WH6alxpFZvpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f72b3e-ba05-4941-c625-83e14e7bfb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('PINECONE_API_KEY')\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "angDJAPU11C_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"rag-project\"  # change if desired\n",
        "\n",
        "\n",
        "pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        " )\n",
        "\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "Ohmirndg2cKX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
      ],
      "metadata": {
        "id": "Rl80wJ7x81aE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMEhbUyu_FKk",
        "outputId": "48722161-17a8-48e5-950f-3dfdc71bc269"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734263241291,\n",
              " 0.01813093200325966]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "duBm4Ut3_lJt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Save\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n"
      ],
      "metadata": {
        "id": "Pkrdb1hy_uxG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEoO2cr_ASbA",
        "outputId": "c290c0e2-eda0-40b7-c79c-93deb95a3812"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['768dc884-029c-4b2a-91ca-5b6b6d95b31e',\n",
              " '72a96adf-ffb6-4533-b5c2-cd4e25dea11b',\n",
              " 'de229558-36d8-48f7-ac89-9e932dee75a3',\n",
              " '9621beb5-a0c1-44e1-8c21-aa9665289556',\n",
              " '8ffff74d-2c52-4ff3-86bf-524eafbee90b',\n",
              " '939fe6d3-ab3e-4767-ac8b-9c57543d8dee',\n",
              " '56a5e346-1b4b-44b6-ae6a-f859e18ca26a',\n",
              " 'e565021f-a4fa-41d6-b80e-f9f171cd9caf',\n",
              " 'eaef07b2-6956-4b7a-b62f-b96f7c898713',\n",
              " 'e550d40b-e2e1-4156-873a-e2fa1d333d78']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Reterive\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbxCXcX3BOzg",
        "outputId": "0d5cfccc-2475-440c-ea1f-d7a439bc9cc2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v6sPk5xBXOE",
        "outputId": "cf291776-d4f0-4f53-8e33-d7583aa01684"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.667716] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "f9v8V6dgJ8_1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answers(query:str):\n",
        "  #Vector Search\n",
        "  vector_results = vector_store.similarity_search(query , k=2)\n",
        "  print(len(vector_results))\n",
        "\n",
        "  #PassnTo Model Vector Result + User Query\n",
        "  final_answer =llm.invoke(f\"answer this user query: {query}, here are some refrences {vector_results}\")\n",
        "\n",
        "\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "VRDnOiefB0Pd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers(\"LangChain provides abstractions to make working with LLMs easy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMipi0fFLFWY",
        "outputId": "3a87fe34-efe9-4f68-e95e-cd07153324b1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"The provided text mentions LangChain as a framework that simplifies working with Large Language Models (LLMs).  However, the included references are tweets that don't directly support this claim. One tweet promotes a competing framework, LangGraph, and the other simply mentions using LangChain in a project.  Therefore, while the initial statement about LangChain's ease of use is plausible (it's a common claim about the framework), the provided references don't offer evidence to support it.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-cd740cc2-b6b3-4253-a855-7fc8a33bba31-0', usage_metadata={'input_tokens': 151, 'output_tokens': 100, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}